{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel, AutoConfig, \n",
    "    AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "from parameters import *\n",
    "from autoencoder import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ds = pd.read_csv(TRAIN_DIR['current'])\n",
    "old_ds = pd.read_csv(TRAIN_DIR['old'])\n",
    "hewlett = pd.read_csv(TRAIN_DIR['hewlett'])\n",
    "\n",
    "label_X = current_ds['full_text'].to_list()\n",
    "unlabled_X = old_ds['texts'].to_list()\n",
    "unlabled_X.extend(hewlett['essay'].to_list())\n",
    "y = current_ds.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPooling(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n",
    "        super(LSTMPooling, self).__init__()\n",
    "        self.num_hidden_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hiddendim_lstm = hiddendim_lstm\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, all_hidden_states):\n",
    "        ## forward\n",
    "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
    "                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "        out, _ = self.lstm(hidden_states, None)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:22<00:00, 11.02it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(_pretrained_model)\n",
    "config.update({'output_hidden_states':True})\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(label_X), 16)):\n",
    "    tail = i + 16 if i+16 < len(label_X) else len(label_X)\n",
    "    \n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        label_X[i:tail],\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    features = features.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features['input_ids'], features['attention_mask'])\n",
    "    \n",
    "    all_hidden_states = torch.stack(outputs[2])\n",
    "    \n",
    "    hiddendim_lstm = 256\n",
    "    pooler = LSTMPooling(config.num_hidden_layers, config.hidden_size, hiddendim_lstm)\n",
    "    pooler = pooler.to(device)\n",
    "    embeddings.append(pooler(all_hidden_states).cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 256)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_np = np.concatenate(embeddings, axis=0)\n",
    "embeddings_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('labeled_data.npy', embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1786/1786 [02:47<00:00, 10.68it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_2 = []\n",
    "\n",
    "for i in tqdm(range(0, len(unlabled_X), 16)):\n",
    "    tail = i + 16 if i+16 < len(unlabled_X) else len(unlabled_X)\n",
    "    \n",
    "    features = tokenizer.batch_encode_plus(\n",
    "        unlabled_X[i:tail],\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    features = features.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features['input_ids'], features['attention_mask'])\n",
    "    \n",
    "    all_hidden_states = torch.stack(outputs[2])\n",
    "    \n",
    "    hiddendim_lstm = 256\n",
    "    pooler = LSTMPooling(config.num_hidden_layers, config.hidden_size, hiddendim_lstm)\n",
    "    pooler = pooler.to(device)\n",
    "    embeddings_2.append(pooler(all_hidden_states).cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28570, 256)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_2_np = np.concatenate(embeddings_2, axis=0)\n",
    "embeddings_2_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('unlabeled_data.npy', embeddings_2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a304760048decebaa3de9d5d5e7ae0455d0e991d0ed55e3d38e6237363fea8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
